Parallel External Sort

The Logic:

	The common sorting technique usually involving loading data into a storage structure, then subsequently sorting out the data in the particular structure. For the aforementioned scenarios, we are assuming that there is an adequate amount of memory that will be ample to hold the entire data collection. However, there are situations where the data size exceeds the memory size, and all of the data will not fit into memory all at once. External sort is a sorting technique to deal with this issue. The idea is to load segments of the data from the original large file into collection structures such that the segment size will not exceed the memory space. We will then sort the smaller segments using the known sorting algorithms (i.e. merge sort, quick sorts), save the sorted segment afterwards to physical files. We then take parts of these smaller segments and incrementally merge them into larger files. We recursively merge segments of two files into a larger file. such that after each merge step, we continuously reduce our file size by half. This process is recursively done until we've reached just one file, which represents the sorted data of the original large data file. We are expected to have multiple processors at our disposal, we will utilize to both sort and merge data concurrently using the Java ForkJoinTask framework.


The design:

	Before delving into the design specifics, we need to first discuss our virtual system environment. The virtual system distribution we have in place assumes many factors. First we assume that the initial partition will be performed inside a server with ample memory to stored the entire large (although we could have just performed a merge sort inside this server and no more work will be needed, and this section of the project will need to be revised such that we expect the distribution system itself to have limits on its memory). The computing cores will have a distributed processor setup with their own separate memory unit. 
	There are 3 key phases in the external sort process. The first phase is to partition the large file into smaller file segments, such that each of these files, can fit its entire data into one. The second phase will perform merge sort on the data of these initial segment files, phase one should have partitioned the files into small files such that these small files will fit into memory (more will be discuss regarding the partitioning logic). The last step is recursively merging the file segments into one result file.
	The goal of the partitioning phase is to split the initial data into a size such that the entire data of these segment files can be loaded into each processor memory. Because we are intending to process the data in parallel, we want the the number of segments to be partitioned in such a way such that each processor will get similar workload. If we do not partitioned our file properly, we end up with a system that has both large process communication overhead and would become less efficient. The partitioning of the initial large data will then need to be an even number that is also able to be scaled by how ever many processors in the system. The way we will implement this specification, is by making the number of segments equal to the number of processors multiply by a power of 2 where the coefficient increases as the dataSize increases. Once we have determined the number of initial file partitions, we will call this number p. We will distribute the data using a round robin approach, where a segment file will receive a data every other p cycle. In this manner, the greatest difference in size between any one file is only one. Hence keeping the work distribution fairly equal among processors.
	Once the initial segment partition is ready, we assume each processor has enough memory to sort each of these files. We will load the entire segment file into memory, use a merge sort to sort the data, and over write the data into the file segment. Each of these files merging steps can be considered a parallelizeable task that we can run concurrently. Hence we will have a specific define a RecursiveAction task for every one of these initial segment file merging. The last step will be to recursively merge the files back using a pair of files at a time. We recursively take two files, load a chunk of their  data into memory, such that the combine data segments from the two file will fit into each processor's memory limits. We sort this segment and writes in into a new file. We continue merging two files into a larger file until we only have one result file.  
  
